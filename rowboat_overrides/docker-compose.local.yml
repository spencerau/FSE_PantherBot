services:
  # rowboat service
  rowboat:
    volumes:                            
      - ./docker-compose.local.yml:/app/docker-compose.local.yml:ro
      - ../data/uploads:/app/uploads
    env_file: ./.env
    environment:
      PROVIDER_TYPE: openai
      PROVIDER_BASE_URL: http://host.docker.internal:11434/v1
      PROVIDER_DEFAULT_MODEL: deepseek-r1:1.5b
      PROVIDER_COPILOT_MODEL: deepseek-r1:1.5b
      EMBEDDING_MODEL: nomic-embed-text
      PROVIDER_PARSE_RESPONSE: "true" 
    depends_on:
      - ollama
      - qdrant_local
      #- litellm

  # rowboat agents
  rowboat_agents:
    env_file: ./.env
    environment:
      PROVIDER_TYPE: openai
      PROVIDER_BASE_URL: http://host.docker.internal:11434/v1
      PROVIDER_DEFAULT_MODEL: deepseek-r1:1.5b
      EMBEDDING_MODEL: nomic-embed-text
      PROVIDER_PARSE_RESPONSE: "true" 
    depends_on:
      - ollama
      - qdrant_local
      # - litellm

  # RAG worker
  rag_files_worker:
    env_file: ./.env
    environment:
      PROVIDER_TYPE: openai
      PROVIDER_BASE_URL: http://host.docker.internal:11434/v1
      PROVIDER_DEFAULT_MODEL: deepseek-r1:1.5b
      EMBEDDING_MODEL: nomic-embed-text
      PROVIDER_PARSE_RESPONSE: "true" 
    volumes:
      - ../data/uploads:/app/uploads
    depends_on:
      - ollama
      - qdrant_local
      # - litellm
      
  # copilot service
  copilot:
    env_file: ./.env
    environment:
      PROVIDER_TYPE: openai
      PROVIDER_BASE_URL: http://host.docker.internal:11434/v1
      PROVIDER_DEFAULT_MODEL: deepseek-r1:1.5b
      PROVIDER_PARSE_RESPONSE: "true" 
      PROVIDER_OPTIONS: '{"temperature":0.7,"stop":["<think>"]}'
      PROVIDER_COMPLETION_PARAMS: '{"response_format":{"type":"text"}}'
    depends_on:
      - ollama
      # - litellm

  # ––– Local Ollama daemon –––
  ollama:
    image: ollama/ollama:0.6.3
    entrypoint: /bin/sh
    command: >
      -c "ollama serve --nofilter &
          echo 'Waiting for Ollama API to be ready...' &&
          until curl -s -f http://localhost:11434/api/tags > /dev/null 2>&1; do
            sleep 2
          done &&
          echo 'Ollama API ready, pulling models...' &&
          ollama pull deepseek-r1:1.5b > /dev/null 2>&1 || echo 'Failed to pull deepseek-r1:1.5b' &&
          ollama pull nomic-embed-text > /dev/null 2>&1 || echo 'Failed to pull nomic-embed-text' &&
          echo 'Pull operations completed, checking models:' &&
          ollama list &&
          echo 'Keeping container alive...' &&
          tail -f /dev/null"
    ports: ["11434:11434"]
    volumes: [ollama_models:/root/.ollama]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 10s
      retries: 15
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: '6'
          memory: 12G 
        reservations:
          cpus: '4'
          memory: 6G
    network_mode: "host"
    # networks:
    #   default:
    #     aliases:
    #       - localhost  # This makes "localhost" point to Ollama

  # copilot:
  #   # Relative to rowboat_overrides directory
  #   env_file: ./.env
  #   depends_on:
  #     - litellm

  # ––– Qdrant vector DB –––
  qdrant_local:
    image: qdrant/qdrant:v1.14.1
    ports: ["6333:6333"]
    volumes: [qdrant_data:/qdrant/storage]

  # ––– LiteLLM proxy –––
  # litellm:
  #   #network_mode: host
  #   image: ghcr.io/berriai/litellm:latest
  #   environment:
  #     PORT: 4000
  #     HOST: "0.0.0.0"
  #     OLLAMA_BASE_URL: "http://localhost:11434"
  #     MODEL_TYPE: "ollama"
  #     LITELLM_PROXY_CONFIG: >
  #       {
  #         "model_list": [
  #           {
  #             "model_name": "ollama/deepseek-r1:1.5b",
  #             "litellm_params": {
  #               "model": "ollama/deepseek-r1:1.5b",
  #               "api_base": "http://localhost:11434"
  #             }
  #           },
  #           {
  #             "model_name": "ollama/nomic-embed-text",
  #             "litellm_params": {
  #               "model": "ollama/nomic-embed-text",
  #               "api_base": "http://localhost:11434"
  #             }
  #           }
  #         ]
  #       }
  #   ports: ["4000:4000"]
  #   depends_on:
  #     - ollama
  #   #network_mode: "service:ollama"
  #   extra_hosts:
  #     - "localhost:172.20.0.3"

  # ─── rowboat service ─────────
  # rowboat:
  #   volumes:                            
  #     - ./docker-compose.local.yml:/app/docker-compose.local.yml:ro
  #     - ../data/uploads:/app/uploads
  #   env_file: ./.env
  #   depends_on:
  #     - ollama
  #     - qdrant_local
  #     - litellm

  # rag_files_worker:
  #   env_file: ./.env
  #   volumes:
  #     - ../data/uploads:/app/uploads
  #   depends_on:
  #     - ollama
  #     - qdrant_local
  #     - litellm

  # rowboat_agents:
  #   env_file: ./.env
  #   depends_on:
  #     - litellm
  #     - ollama
  #     - qdrant_local

  mongo:
    image: mongo:latest
    command: mongod --bind_ip_all
    environment:
      MONGO_INITDB_ROOT_USERNAME: ""
      MONGO_INITDB_ROOT_PASSWORD: ""
      MONGODB_FORCE_CLEAN_START: "true"
    volumes:
      - mongo_data:/data/db
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017 --quiet
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

volumes:
  ollama_models:
  qdrant_data:
  mongo_data:

networks:
  default:
    name: rowboat-network
